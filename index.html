<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GOBA-AI-Labs — Making Large AI Accessible</title>
  <meta name="description" content="Open-source MoE compression. Run 400B+ parameter AI models on consumer hardware with 24GB RAM.">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="icon" href="favicon.png" type="image/png">
  <link rel="stylesheet" href="style.css">
  <script src="theme.js"></script>
</head>
<body>

<!-- Nav -->
<nav>
  <div class="nav-inner">
    <a href="/" class="logo">
      <img id="nav-logo" src="logo.png" alt="GOBA AI Labs" class="nav-logo-img">
    </a>
    <div class="nav-right">
      <div class="nav-links">
        <a href="#models" data-i18n="nav.models">Models</a>
        <a href="#tools" data-i18n="nav.tools">Tools</a>
        <a href="#technology" data-i18n="nav.technology">Technology</a>
        <a href="paper/" data-i18n="nav.paper">Paper</a>
        <a href="https://huggingface.co/goba-ai-labs" target="_blank" data-i18n="nav.huggingface">HuggingFace</a>
        <a href="https://ko-fi.com/gobaailabs" target="_blank" class="btn-nav" data-i18n="nav.support">Support Us</a>
      </div>
      <div class="nav-controls">
        <button id="theme-toggle" aria-label="Toggle theme"></button>
        <div class="lang-switcher">
          <button class="lang-option active" data-lang="en">EN</button>
          <button class="lang-option" data-lang="ja">JA</button>
          <button class="lang-option" data-lang="zh-CN">ZH</button>
          <button class="lang-option" data-lang="zh-TW">TW</button>
          <button class="lang-option" data-lang="ko">KO</button>
        </div>
      </div>
    </div>
  </div>
</nav>

<!-- Hero -->
<section class="hero">
  <div class="container">
    <h1 data-i18n="hero.title">Half the experts.<br>Full quality.</h1>
    <p class="subtitle" data-i18n="hero.subtitle">
      We remove up to 50% of MoE experts while preserving benchmark scores.
      Proven on models up to 80B — our goal is 400B+ on consumer hardware.
    </p>
    <div class="hero-actions">
      <a href="#models" class="btn btn-primary" data-i18n="hero.browse">Browse Models</a>
      <a href="https://huggingface.co/goba-ai-labs" target="_blank" class="btn btn-secondary" data-i18n="hero.huggingface">HuggingFace</a>
    </div>
    <div class="hero-stat">
      <div class="stat">
        <span class="stat-value">16.8 GB</span>
        <span class="stat-arrow">&rarr;</span>
        <span class="stat-value highlight">13.7 GB</span>
      </div>
      <span class="stat-label" data-i18n="hero.stat">Qwen3.5-35B-A3B — 20% expert pruning, MMLU 80%, fits 24GB Mac GPU</span>
    </div>
  </div>
</section>

<!-- Models -->
<section id="models" class="models">
  <div class="container">
    <h2 data-i18n="models.title">PrunedHub Models</h2>
    <p class="section-desc" data-i18n="models.desc">8 pruned MoE models ready to run. All compatible with llama.cpp. Qwen3/3.5 also work with moe-stream.</p>
    <div class="model-grid">

      <a href="https://huggingface.co/GOBA-AI-Labs/PrunedHub-Qwen3.5-35B-A3B-80pct" target="_blank" class="model-card featured">
        <div class="model-badge new" data-i18n="model.new">NEW</div>
        <h3>Qwen3.5-35B-A3B-80pct</h3>
        <p class="model-base">Qwen/Qwen3.5-35B-A3B</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label">Size</span>
            <span class="value">13.7 GB</span>
          </div>
          <div class="model-stat-item">
            <span class="label">MMLU</span>
            <span class="value">80%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">GSM8K</span>
            <span class="value">82%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">LCB Easy</span>
            <span class="value">83%</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.qwen35.desc">DeltaNet hybrid, 256 experts. Pruned within 24h of release. Full GPU-resident on 24GB Mac. MMLU -1pp.</p>
      </a>

      <a href="https://huggingface.co/goba-ai-labs/PrunedHub-GPT-OSS-20B-28x" target="_blank" class="model-card">
        <div class="model-badge lossless" data-i18n="model.lossless">Lossless</div>
        <h3>GPT-OSS-20B-28x</h3>
        <p class="model-base">openai/gpt-oss-20b</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label">Size</span>
            <span class="value">10.4 GB</span>
          </div>
          <div class="model-stat-item">
            <span class="label">MMLU</span>
            <span class="value">78%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">HumanEval</span>
            <span class="value">78%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">GSM8K</span>
            <span class="value">92%</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.gptoss28.desc">Zero quality loss across all benchmarks. MMLU 78%, HumanEval 78%, GSM8K 92%. Fits 16GB RAM.</p>
      </a>

      <a href="https://huggingface.co/goba-ai-labs/PrunedHub-Qwen3-Coder-Next-50pct" target="_blank" class="model-card">
        <div class="model-badge extreme" data-i18n="model.50pruned">50% Pruned</div>
        <h3>Qwen3-Coder-Next-50pct</h3>
        <p class="model-base">Qwen/Qwen3-Coder-Next</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label">Size</span>
            <span class="value">24.4 GB</span>
          </div>
          <div class="model-stat-item">
            <span class="label">MMLU</span>
            <span class="value">72%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">Reduction</span>
            <span class="value">-45.8%</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.qwen80b.desc">80B model compressed to 24GB. MMLU 72%. At 44% pruning (27.7 GB): LCB Easy 83%, HumanEval 72%.</p>
      </a>

      <a href="https://huggingface.co/goba-ai-labs/PrunedHub-Qwen3-30B-A3B-JP-80pct" target="_blank" class="model-card">
        <div class="model-badge ja" data-i18n="model.japanese">Japanese</div>
        <h3>Qwen3-30B-A3B-JP-80pct</h3>
        <p class="model-base">Qwen/Qwen3-30B-A3B</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label">Size</span>
            <span class="value">14.0 GB</span>
          </div>
          <div class="model-stat-item">
            <span class="label">MMLU</span>
            <span class="value">79%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">Reduction</span>
            <span class="value">-19.1%</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.jp30b.desc">Language-aware pruning preserves Japanese quality. Thinking-ON: MMLU 79%, JA 90%.</p>
      </a>

      <a href="https://huggingface.co/goba-ai-labs/PrunedHub-GPT-OSS-20B-27x-Zerobias" target="_blank" class="model-card">
        <div class="model-badge experimental" data-i18n="model.experimental">Experimental</div>
        <h3>GPT-OSS-20B-27x-Zerobias</h3>
        <p class="model-base">openai/gpt-oss-20b</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label">Size</span>
            <span class="value">~9.4 GB</span>
          </div>
          <div class="model-stat-item">
            <span class="label">MMLU</span>
            <span class="value">77%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">Reduction</span>
            <span class="value">-19.5%</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.zerobias.desc">Router optimization recovers quality at the pruning cliff. -1pp with 15.6% fewer experts.</p>
      </a>

      <a href="https://huggingface.co/GOBA-AI-Labs/PrunedHub-Qwen3-30B-A3B-EN-80pct-MxMoE" target="_blank" class="model-card">
        <div class="model-badge mxmoe" data-i18n="model.mxmoe">MxMoE</div>
        <h3>Qwen3-30B-EN-MxMoE</h3>
        <p class="model-base">Qwen/Qwen3-30B-A3B</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label">Size</span>
            <span class="value">13.5 GB</span>
          </div>
          <div class="model-stat-item">
            <span class="label">MMLU</span>
            <span class="value">70%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">GSM8K</span>
            <span class="value">94%</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.enmxmoe.desc">EN-optimized pruning + mixed quantization (Q5K/Q4K/Q3K). 22% smaller, GSM8K 94%.</p>
      </a>

      <a href="https://huggingface.co/GOBA-AI-Labs/PrunedHub-Qwen3-30B-A3B-JP-80pct-MxMoE" target="_blank" class="model-card">
        <div class="model-badge ja" data-i18n="model.japanese">Japanese</div>
        <h3>Qwen3-30B-JP-MxMoE</h3>
        <p class="model-base">Qwen/Qwen3-30B-A3B</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label">Size</span>
            <span class="value">13.5 GB</span>
          </div>
          <div class="model-stat-item">
            <span class="label">MMLU</span>
            <span class="value">73%</span>
          </div>
          <div class="model-stat-item">
            <span class="label">JA Quality</span>
            <span class="value">85%</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.jpmxmoe.desc">JP language-aware pruning + MxMoE. GSM8K 96%, Japanese quality preserved.</p>
      </a>

    </div>

    <h2 class="subsection-title" data-i18n="models.goba.title">GOBA Models</h2>
    <p class="section-desc" data-i18n="models.goba.desc">Domain-tuned models using our Expert-as-Adapter technique. Pruned expert slots repurposed for specialization.</p>
    <div class="model-grid">

      <a href="https://huggingface.co/GOBA-AI-Labs/GOBA-OLMoE-Expert-Tuned" target="_blank" class="model-card">
        <div class="model-badge expert-tuned" data-i18n="model.experttuned">Expert Tuned</div>
        <h3>OLMoE-1B-7B Expert Tuned</h3>
        <p class="model-base">allenai/OLMoE-1B-7B</p>
        <div class="model-stats">
          <div class="model-stat-item">
            <span class="label" data-i18n="model.olmoe.domains">Domains</span>
            <span class="value">JA / Finance / Code</span>
          </div>
          <div class="model-stat-item">
            <span class="label">JMMLU</span>
            <span class="value">+4.5pp</span>
          </div>
          <div class="model-stat-item">
            <span class="label">HumanEval+</span>
            <span class="value">+10pp</span>
          </div>
        </div>
        <p class="model-desc" data-i18n="model.olmoe.desc">3 domain-tuned variants via Expert-as-Adapter. JA +4.5pp JMMLU, Code +10pp HumanEval+, Finance +16pp fraud detection.</p>
      </a>

    </div>

    <div class="research-highlights">
      <h3 data-i18n="models.research.title">Research Highlights</h3>
      <div class="highlight-grid">
        <div class="highlight-item">
          <span class="highlight-label" data-i18n="models.research.80b">80B Code Generation Finding</span>
          <p data-i18n="models.research.80b.desc">50% expert pruning preserves MMLU 76% but destroys code generation (HumanEval 0%). Code generation uses 84% of experts &mdash; a distributed capability that resists pruning.</p>
        </div>
        <div class="highlight-item">
          <span class="highlight-label" data-i18n="models.research.qwen35">Qwen3.5-35B-A3B: Day-1 Pruning</span>
          <p data-i18n="models.research.qwen35.desc">Pruned within 24 hours of release. 5 methods compared on 256-expert DeltaNet hybrid. Weight-based pruning wins: MMLU 80% (-1pp), LiveCodeBench Easy 83.1% (142 problems). Published on HuggingFace.</p>
        </div>
      </div>
    </div>

  </div>
</section>

<!-- Tools -->
<section id="tools" class="tools">
  <div class="container">
    <h2 data-i18n="tools.title">Inference Tools</h2>
    <p class="section-desc" data-i18n="tools.desc">Run pruned MoE models on consumer hardware.</p>
    <a href="https://github.com/GOBA-AI-Labs/moe-stream" target="_blank" class="tool-card">
      <div class="tool-header">
        <h3>moe-stream</h3>
        <span class="tool-lang">Rust</span>
      </div>
      <p class="tool-tagline" data-i18n="tools.moestream.tagline">SSD-streaming MoE inference engine for Apple Silicon, CUDA &amp; Linux</p>
      <div class="tool-features">
        <div class="tool-feature" data-i18n="tools.moestream.ssd">80B models with 4GB RAM via NVMe SSD streaming</div>
        <div class="tool-feature" data-i18n="tools.moestream.adaptive">Layer-adaptive pruning support (experts_per_layer)</div>
        <div class="tool-feature" data-i18n="tools.moestream.speed">Q4 quantized matmul &mdash; +79% speedup</div>
        <div class="tool-feature" data-i18n="tools.moestream.metal">Metal GPU + CUDA + CPU hybrid inference</div>
        <div class="tool-feature" data-i18n="tools.moestream.cicd">CI/CD with pre-built binaries for macOS &amp; Linux</div>
        <div class="tool-feature" data-i18n="tools.moestream.python">Python bindings (PyO3)</div>
      </div>
      <div class="tool-stats">
        <span>~55 tok/s GPU-resident</span>
        <span>~2 tok/s SSD streaming</span>
        <span>MIT / Apache-2.0</span>
      </div>
      <span class="tool-cta" data-i18n="tools.moestream.github">View on GitHub</span>
    </a>
  </div>
</section>

<!-- Technology -->
<section id="technology" class="technology">
  <div class="container">
    <h2 data-i18n="tech.title">How It Works</h2>
    <p class="section-desc" data-i18n="tech.desc">Expert pruning, not aggressive quantization.</p>
    <div class="tech-grid">

      <div class="tech-card">
        <div class="tech-icon">&#x1F50D;</div>
        <h3 data-i18n="tech.calibration">Calibration-Based Scoring</h3>
        <p data-i18n="tech.calibration.desc">Expert importance measured through actual inference on diverse workloads. Significantly more accurate than static weight analysis.</p>
      </div>

      <div class="tech-card">
        <div class="tech-icon">&#x1F4CA;</div>
        <h3 data-i18n="tech.adaptive">Layer-Adaptive Allocation</h3>
        <p data-i18n="tech.adaptive.desc">Each layer retains a dynamically determined number of experts. Some layers are more sensitive to pruning — adaptive allocation preserves quality where it matters.</p>
      </div>

      <div class="tech-card">
        <div class="tech-icon">&#x1F30F;</div>
        <h3 data-i18n="tech.language">Language-Aware Optimization</h3>
        <p data-i18n="tech.language.desc">Automatic detection and protection of language-specialized experts. Japanese, Chinese, and other language capabilities preserved during compression.</p>
      </div>

      <div class="tech-card">
        <div class="tech-icon">&#x26A1;</div>
        <h3 data-i18n="tech.zerobias">Zerobias Router Optimization</h3>
        <p data-i18n="tech.zerobias.desc">Post-pruning router bias correction extends the lossless compression frontier. Zero cost, no retraining required.</p>
      </div>

      <div class="tech-card">
        <div class="tech-icon">&#x1F9E9;</div>
        <h3 data-i18n="tech.adapter">Expert-as-Adapter</h3>
        <p data-i18n="tech.adapter.desc">Pruned expert slots become domain adapters. A LoRA alternative using the MoE architecture itself &mdash; no external modules, native router integration.</p>
      </div>

    </div>

    <div class="comparison">
      <h3 data-i18n="comp.title">Expert Pruning vs Quantization</h3>
      <div class="comparison-table">
        <div class="comp-row comp-header">
          <span></span>
          <span data-i18n="comp.pruning">Expert Pruning</span>
          <span data-i18n="comp.quantization">Q2 Quantization</span>
        </div>
        <div class="comp-row">
          <span data-i18n="comp.approach">Approach</span>
          <span data-i18n="comp.pruning.approach">Remove redundant experts entirely</span>
          <span data-i18n="comp.quant.approach">Reduce precision of all weights</span>
        </div>
        <div class="comp-row">
          <span data-i18n="comp.quality">Quality impact</span>
          <span class="good" data-i18n="comp.pruning.quality">Targeted, minimal</span>
          <span class="bad" data-i18n="comp.quant.quality">Uniform degradation</span>
        </div>
        <div class="comp-row">
          <span data-i18n="comp.24gb">~24 GB model quality</span>
          <span class="good" data-i18n="comp.pruning.24gb">MMLU 72%</span>
          <span class="bad" data-i18n="comp.quant.24gb">MMLU ~55–60%</span>
        </div>
        <div class="comp-row">
          <span data-i18n="comp.precision">Remaining precision</span>
          <span class="good" data-i18n="comp.pruning.precision">Full Q4 precision</span>
          <span class="bad" data-i18n="comp.quant.precision">2-bit precision</span>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Vision -->
<section id="vision" class="vision">
  <div class="container">
    <h2 data-i18n="vision.title">Where This Is Going</h2>
    <p class="section-desc" data-i18n="vision.desc">Expert pruning is just the beginning. Here's what becomes possible.</p>
    <div class="tech-grid">

      <div class="tech-card">
        <div class="tech-icon">&#x1F4BB;</div>
        <h3 data-i18n="vision.hardware">Consumer Hardware</h3>
        <p data-i18n="vision.hardware.desc">Frontier-class MoE models on a laptop with 24GB RAM — no $10K+ server GPU required. Democratizing access to the most capable AI.</p>
      </div>

      <div class="tech-card">
        <div class="tech-icon">&#x1F50B;</div>
        <h3 data-i18n="vision.energy">Lower Energy Cost</h3>
        <p data-i18n="vision.energy.desc">Fewer experts means fewer FLOPs per token. Data centers could serve the same quality at half the compute — significant power savings at scale.</p>
      </div>

      <div class="tech-card">
        <div class="tech-icon">&#x1F680;</div>
        <h3 data-i18n="vision.rl">Post-Pruning RL</h3>
        <p data-i18n="vision.rl.desc">Pruned models create headroom for targeted reinforcement learning. Same size, potentially better performance — quality amplification, not just preservation.</p>
      </div>

      <div class="tech-card">
        <div class="tech-icon">&#x1F310;</div>
        <h3 data-i18n="vision.edge">Edge Deployment</h3>
        <p data-i18n="vision.edge.desc">MoE models become viable for on-device inference. Private, fast, and offline — bringing expert-level AI to phones and embedded systems.</p>
      </div>

    </div>
  </div>
</section>

<!-- CTA -->
<section class="cta">
  <div class="container">
    <h2 data-i18n="cta.title">Built on a MacBook. Funded by You.</h2>
    <p class="cta-detail" data-i18n="cta.detail">This entire research — 29+ phases, 6 model families, 8 published models, 100+ experiments — was conducted on a single MacBook Pro (M4 Pro, 24GB RAM, 512GB SSD). No data center. No corporate funding.</p>
    <p class="cta-detail" data-i18n="cta.funding">Your support directly funds GPU access to test on larger models, cloud compute for comprehensive benchmarking, and time to continue open-source research.</p>
    <a href="https://ko-fi.com/gobaailabs" target="_blank" class="btn btn-primary btn-large" data-i18n="cta.button">Support on Ko-fi</a>
  </div>
</section>

<!-- Footer -->
<footer>
  <div class="container footer-inner">
    <span class="footer-name" data-i18n="footer.name">GOBA-AI-Labs</span>
    <div class="footer-links">
      <a href="https://github.com/GOBA-AI-Labs" target="_blank">GitHub</a>
      <a href="https://huggingface.co/goba-ai-labs" target="_blank">HuggingFace</a>
      <a href="https://ko-fi.com/gobaailabs" target="_blank">Ko-fi</a>
    </div>
  </div>
</footer>

<script src="i18n.js"></script>
</body>
</html>
