(function () {
  const KEY = 'goba-lang';

  const translations = {
    en: {
      'nav.models': 'Models',
      'nav.technology': 'Technology',
      'nav.huggingface': 'HuggingFace',
      'nav.support': 'Support Us',
      'hero.title': 'Half the experts.<br>Full quality.',
      'hero.subtitle': 'We remove up to 50% of MoE experts while preserving benchmark scores. Proven on models up to 80B \u2014 our goal is 400B+ on consumer hardware.',
      'hero.browse': 'Browse Models',
      'hero.huggingface': 'HuggingFace',
      'hero.stat': 'Qwen3-Coder-Next (80B) \u2014 50% expert pruning, MMLU 72%',
      'nav.tools': 'Tools',
      'models.title': 'PrunedHub Models',
      'models.desc': 'Pruned MoE models ready to run. GPT-OSS models work with llama.cpp; Qwen3 models require moe-stream.',
      'tools.title': 'Inference Tools',
      'tools.desc': 'Run pruned MoE models on consumer hardware.',
      'tools.moestream.tagline': 'SSD-streaming MoE inference engine for Apple Silicon & Linux',
      'tools.moestream.ssd': '80B models with 4GB RAM via NVMe SSD streaming',
      'tools.moestream.adaptive': 'Layer-adaptive pruning support (experts_per_layer)',
      'tools.moestream.speed': 'Q4 quantized matmul \u2014 +79% speedup',
      'tools.moestream.metal': 'Metal GPU + CPU hybrid inference',
      'tools.moestream.github': 'View on GitHub',
      'model.lossless': 'Lossless',
      'model.50pruned': '50% Pruned',
      'model.japanese': 'Japanese',
      'model.experimental': 'Experimental',
      'model.gptoss28.desc': 'Zero quality loss across all benchmarks. MMLU 78%, HumanEval 78%, GSM8K 92%. Fits 16GB RAM.',
      'model.qwen80b.desc': '80B model compressed to 24GB. MMLU 72%. At 44% pruning (27.7 GB): LCB Easy 83%, HumanEval 72%.',
      'model.jp30b.desc': 'Language-aware pruning preserves Japanese quality. Thinking-ON: MMLU 79%, JA 90%.',
      'model.zerobias.desc': 'Router optimization recovers quality at the pruning cliff. -1pp with 15.6% fewer experts.',
      'tech.title': 'How It Works',
      'tech.desc': 'Expert pruning, not aggressive quantization.',
      'tech.calibration': 'Calibration-Based Scoring',
      'tech.calibration.desc': 'Expert importance measured through actual inference on diverse workloads. Significantly more accurate than static weight analysis.',
      'tech.adaptive': 'Layer-Adaptive Allocation',
      'tech.adaptive.desc': 'Each layer retains a dynamically determined number of experts. Some layers are more sensitive to pruning \u2014 adaptive allocation preserves quality where it matters.',
      'tech.language': 'Language-Aware Optimization',
      'tech.language.desc': 'Automatic detection and protection of language-specialized experts. Japanese, Chinese, and other language capabilities preserved during compression.',
      'tech.zerobias': 'Zerobias Router Optimization',
      'tech.zerobias.desc': 'Post-pruning router bias correction extends the lossless compression frontier. Zero cost, no retraining required.',
      'comp.title': 'Expert Pruning vs Quantization',
      'comp.pruning': 'Expert Pruning',
      'comp.quantization': 'Q2 Quantization',
      'comp.approach': 'Approach',
      'comp.pruning.approach': 'Remove redundant experts entirely',
      'comp.quant.approach': 'Reduce precision of all weights',
      'comp.quality': 'Quality impact',
      'comp.pruning.quality': 'Targeted, minimal',
      'comp.quant.quality': 'Uniform degradation',
      'comp.24gb': '~24 GB model quality',
      'comp.pruning.24gb': 'MMLU 72%',
      'comp.quant.24gb': 'MMLU ~55\u201360%',
      'comp.precision': 'Remaining precision',
      'comp.pruning.precision': 'Full Q4 precision',
      'comp.quant.precision': '2-bit precision',
      'vision.title': 'Where This Is Going',
      'vision.desc': 'Expert pruning is just the beginning. Here\'s what becomes possible.',
      'vision.hardware': 'Consumer Hardware',
      'vision.hardware.desc': 'Frontier-class MoE models on a laptop with 24GB RAM \u2014 no $10K+ server GPU required. Democratizing access to the most capable AI.',
      'vision.energy': 'Lower Energy Cost',
      'vision.energy.desc': 'Fewer experts means fewer FLOPs per token. Data centers could serve the same quality at half the compute \u2014 significant power savings at scale.',
      'vision.rl': 'Post-Pruning RL',
      'vision.rl.desc': 'Pruned models create headroom for targeted reinforcement learning. Same size, potentially better performance \u2014 quality amplification, not just preservation.',
      'vision.edge': 'Edge Deployment',
      'vision.edge.desc': 'MoE models become viable for on-device inference. Private, fast, and offline \u2014 bringing expert-level AI to phones and embedded systems.',
      'cta.title': 'Built on a MacBook. Funded by You.',
      'cta.detail': 'This entire research \u2014 22 phases, 4 models, 50+ experiments \u2014 was conducted on a single MacBook Pro (M4 Pro, 24GB RAM, 512GB SSD). No data center. No corporate funding.',
      'cta.funding': 'Your support directly funds GPU access to test on larger models, cloud compute for comprehensive benchmarking, and time to continue open-source research.',
      'cta.button': 'Support on Ko-fi',
      'footer.name': 'GOBA-AI-Labs',
    },
    ja: {
      'nav.models': '\u30E2\u30C7\u30EB',
      'nav.technology': '\u6280\u8853',
      'nav.huggingface': 'HuggingFace',
      'nav.support': '\u652F\u63F4\u3059\u308B',
      'hero.title': 'Expert\u534A\u5206\u3002<br>\u54C1\u8CEA\u305D\u306E\u307E\u307E\u3002',
      'hero.subtitle': 'MoE\u30E2\u30C7\u30EB\u306Eexpert\u3092\u6700\u592750%\u524A\u9664\u3057\u3001\u30D9\u30F3\u30C1\u30DE\u30FC\u30AF\u30B9\u30B3\u30A2\u3092\u7DAD\u6301\u300280B\u30E2\u30C7\u30EB\u3067\u5B9F\u8A3C\u6E08\u307F \u2014 \u76EE\u6A19\u306F400B+\u3092\u30B3\u30F3\u30B7\u30E5\u30FC\u30DE\u30FC\u30CF\u30FC\u30C9\u30A6\u30A7\u30A2\u3067\u3002',
      'hero.browse': '\u30E2\u30C7\u30EB\u4E00\u89A7',
      'hero.huggingface': 'HuggingFace',
      'hero.stat': 'Qwen3-Coder-Next (80B) \u2014 50% expert pruning, MMLU 72%',
      'nav.tools': '\u30C4\u30FC\u30EB',
      'models.title': 'PrunedHub \u30E2\u30C7\u30EB',
      'models.desc': 'Pruned MoE\u30E2\u30C7\u30EB\u3002GPT-OSS\u306Fllama.cpp\u5BFE\u5FDC\u3001Qwen3\u306Fmoe-stream\u304C\u5FC5\u8981\u3002',
      'tools.title': '\u63A8\u8AD6\u30C4\u30FC\u30EB',
      'tools.desc': '\u30B3\u30F3\u30B7\u30E5\u30FC\u30DE\u30FC\u30CF\u30FC\u30C9\u30A6\u30A7\u30A2\u3067pruned MoE\u30E2\u30C7\u30EB\u3092\u5B9F\u884C\u3002',
      'tools.moestream.tagline': 'Apple Silicon & Linux\u5411\u3051SSD\u30B9\u30C8\u30EA\u30FC\u30DF\u30F3\u30B0MoE\u63A8\u8AD6\u30A8\u30F3\u30B8\u30F3',
      'tools.moestream.ssd': '80B\u30E2\u30C7\u30EB\u30924GB RAM\u3067\u5B9F\u884C\uFF08NVMe SSD\u30B9\u30C8\u30EA\u30FC\u30DF\u30F3\u30B0\uFF09',
      'tools.moestream.adaptive': '\u30EC\u30A4\u30E4\u30FC\u9069\u5FDC\u578Bpruning\u5BFE\u5FDC\uFF08experts_per_layer\uFF09',
      'tools.moestream.speed': 'Q4\u91CF\u5B50\u5316matmul \u2014 +79%\u9AD8\u901F\u5316',
      'tools.moestream.metal': 'Metal GPU + CPU\u30CF\u30A4\u30D6\u30EA\u30C3\u30C9\u63A8\u8AD6',
      'tools.moestream.github': 'GitHub\u3067\u898B\u308B',
      'model.lossless': '\u30ED\u30B9\u30EC\u30B9',
      'model.50pruned': '50% \u524A\u6E1B',
      'model.japanese': '\u65E5\u672C\u8A9E\u5BFE\u5FDC',
      'model.experimental': '\u5B9F\u9A13\u7684',
      'model.gptoss28.desc': '全ベンチマークで品質劣化ゼロ。MMLU 78%、HumanEval 78%、GSM8K 92%。16GB RAMに収まる。',
      'model.qwen80b.desc': '80Bモデルを24GBに圧縮。MMLU 72%。44% pruning (27.7 GB)時: LCB Easy 83%、HumanEval 72%。',
      'model.jp30b.desc': '\u8A00\u8A9E\u5BFE\u5FDC\u30D7\u30EB\u30FC\u30CB\u30F3\u30B0\u3067\u65E5\u672C\u8A9E\u54C1\u8CEA\u3092\u7DAD\u6301\u3002Thinking-ON: MMLU 79%, JA 90%\u3002',
      'model.zerobias.desc': '\u30EB\u30FC\u30BF\u30FC\u6700\u9069\u5316\u3067pruning cliff\u306E\u54C1\u8CEA\u3092\u56DE\u5FA9\u3002expert 15.6%\u524A\u6E1B\u3067-1pp\u3002',
      'tech.title': '\u6280\u8853\u306E\u4ED5\u7D44\u307F',
      'tech.desc': '\u904E\u5EA6\u306A\u91CF\u5B50\u5316\u3067\u306F\u306A\u304F\u3001expert\u30D7\u30EB\u30FC\u30CB\u30F3\u30B0\u3002',
      'tech.calibration': '\u30AD\u30E3\u30EA\u30D6\u30EC\u30FC\u30B7\u30E7\u30F3\u30D9\u30FC\u30B9\u30B9\u30B3\u30A2\u30EA\u30F3\u30B0',
      'tech.calibration.desc': '\u591A\u69D8\u306A\u30EF\u30FC\u30AF\u30ED\u30FC\u30C9\u3067\u306E\u5B9F\u969B\u306E\u63A8\u8AD6\u3092\u901A\u3058\u3066expert\u306E\u91CD\u8981\u5EA6\u3092\u6E2C\u5B9A\u3002\u9759\u7684\u91CD\u307F\u5206\u6790\u3088\u308A\u5927\u5E45\u306B\u9AD8\u7CBE\u5EA6\u3002',
      'tech.adaptive': '\u30EC\u30A4\u30E4\u30FC\u9069\u5FDC\u578B\u5272\u5F53',
      'tech.adaptive.desc': '\u5404\u30EC\u30A4\u30E4\u30FC\u304C\u52D5\u7684\u306B\u6C7A\u5B9A\u3055\u308C\u305F\u6570\u306Eexpert\u3092\u4FDD\u6301\u3002\u30D7\u30EB\u30FC\u30CB\u30F3\u30B0\u306B\u654F\u611F\u306A\u30EC\u30A4\u30E4\u30FC\u306E\u54C1\u8CEA\u3092\u512A\u5148\u7684\u306B\u4FDD\u5168\u3002',
      'tech.language': '\u8A00\u8A9E\u5BFE\u5FDC\u6700\u9069\u5316',
      'tech.language.desc': '\u8A00\u8A9E\u7279\u5316expert\u306E\u81EA\u52D5\u691C\u51FA\u3068\u4FDD\u8B77\u3002\u65E5\u672C\u8A9E\u30FB\u4E2D\u56FD\u8A9E\u7B49\u306E\u8A00\u8A9E\u80FD\u529B\u3092\u5727\u7E2E\u6642\u306B\u7DAD\u6301\u3002',
      'tech.zerobias': 'Zerobias\u30EB\u30FC\u30BF\u30FC\u6700\u9069\u5316',
      'tech.zerobias.desc': '\u30D7\u30EB\u30FC\u30CB\u30F3\u30B0\u5F8C\u306E\u30EB\u30FC\u30BF\u30FCbias\u88DC\u6B63\u3067\u30ED\u30B9\u30EC\u30B9\u5727\u7E2E\u306E\u9650\u754C\u3092\u62E1\u5F35\u3002\u30B3\u30B9\u30C8\u30BC\u30ED\u3001\u518D\u8A13\u7DF4\u4E0D\u8981\u3002',
      'comp.title': 'Expert\u30D7\u30EB\u30FC\u30CB\u30F3\u30B0 vs \u91CF\u5B50\u5316',
      'comp.pruning': 'Expert\u30D7\u30EB\u30FC\u30CB\u30F3\u30B0',
      'comp.quantization': 'Q2\u91CF\u5B50\u5316',
      'comp.approach': '\u30A2\u30D7\u30ED\u30FC\u30C1',
      'comp.pruning.approach': '\u5197\u9577\u306Aexpert\u3092\u5B8C\u5168\u306B\u524A\u9664',
      'comp.quant.approach': '\u5168\u91CD\u307F\u306E\u7CBE\u5EA6\u3092\u524A\u6E1B',
      'comp.quality': '\u54C1\u8CEA\u3078\u306E\u5F71\u97FF',
      'comp.pruning.quality': '\u7684\u3092\u7D5E\u3063\u305F\u6700\u5C0F\u9650\u306E\u5F71\u97FF',
      'comp.quant.quality': '\u4E00\u69D8\u306A\u54C1\u8CEA\u52A3\u5316',
      'comp.24gb': '~24 GB\u30E2\u30C7\u30EB\u54C1\u8CEA',
      'comp.pruning.24gb': 'MMLU 72%',
      'comp.quant.24gb': 'MMLU ~55\uFF5E60%',
      'comp.precision': '\u6B8B\u5B58\u7CBE\u5EA6',
      'comp.pruning.precision': 'Q4\u30D5\u30EB\u7CBE\u5EA6',
      'comp.quant.precision': '2\u30D3\u30C3\u30C8\u7CBE\u5EA6',
      'vision.title': 'この先にあるもの',
      'vision.desc': 'Expert pruningは始まりに過ぎません。ここから何が可能になるか。',
      'vision.hardware': 'コンシューマーハードウェア',
      'vision.hardware.desc': 'フロンティアクラスのMoEモデルを24GB RAMのノートPCで \u2014 10万円超のサーバーGPU不要。最高性能AIへのアクセスを民主化。',
      'vision.energy': '低消費電力',
      'vision.energy.desc': 'expertが減ればトークンあたりのFLOPsも減少。データセンターは同等品質を半分の計算量で提供可能 \u2014 大規模な省電力を実現。',
      'vision.rl': 'プルーニング後の強化学習',
      'vision.rl.desc': 'プルーニング済みモデルは強化学習の余地を生む。同サイズでより高い性能 \u2014 品質の保持だけでなく増幅の可能性。',
      'vision.edge': 'エッジデプロイメント',
      'vision.edge.desc': 'MoEモデルがオンデバイス推論で実用可能に。プライベート、高速、オフライン \u2014 expertレベルのAIをスマートフォンや組込みシステムへ。',
      'cta.title': 'MacBookで生まれた研究。あなたの支援で加速する。',
      'cta.detail': 'この研究のすべて \u2014 22フェーズ、4モデル、50以上の実験 \u2014 は1台のMacBook Pro（M4 Pro、24GB RAM、512GB SSD）で実施されました。データセンターなし。企業資金なし。',
      'cta.funding': 'ご支援はより大きなモデルでのテスト用GPU、包括的なベンチマーク用クラウドコンピュート、そしてオープンソース研究を続ける時間に直接充てられます。',
      'cta.button': 'Ko-fiで支援',
      'footer.name': 'GOBA-AI-Labs',
    },
    'zh-CN': {
      'nav.models': '\u6A21\u578B',
      'nav.technology': '\u6280\u672F',
      'nav.huggingface': 'HuggingFace',
      'nav.support': '\u652F\u6301\u6211\u4EEC',
      'hero.title': '\u4E13\u5BB6\u51CF\u534A\u3002<br>\u8D28\u91CF\u4E0D\u53D8\u3002',
      'hero.subtitle': '\u6211\u4EEC\u79FB\u9664MoE\u6A21\u578B\u591A\u8FBE50%\u7684\u4E13\u5BB6\uFF0C\u540C\u65F6\u4FDD\u6301\u57FA\u51C6\u6D4B\u8BD5\u5206\u6570\u3002\u5DF2\u5728\u6700\u5927 80B \u6A21\u578B\u4E0A\u9A8C\u8BC1 \u2014 \u76EE\u6807\u662F\u8BA9400B+\u6A21\u578B\u5728\u6D88\u8D39\u7EA7\u786C\u4EF6\u4E0A\u8FD0\u884C\u3002',
      'hero.browse': '\u6D4F\u89C8\u6A21\u578B',
      'hero.huggingface': 'HuggingFace',
      'hero.stat': 'Qwen3-Coder-Next (80B) \u2014 50% expert pruning, MMLU 72%',
      'nav.tools': '\u5DE5\u5177',
      'models.title': 'PrunedHub \u6A21\u578B',
      'models.desc': '\u526A\u679D\u540E\u7684MoE\u6A21\u578B\u3002GPT-OSS\u652F\u6301llama.cpp\uFF0CQwen3\u9700\u8981moe-stream\u3002',
      'tools.title': '\u63A8\u7406\u5DE5\u5177',
      'tools.desc': '\u5728\u6D88\u8D39\u7EA7\u786C\u4EF6\u4E0A\u8FD0\u884C\u526A\u679D\u540E\u7684MoE\u6A21\u578B\u3002',
      'tools.moestream.tagline': '\u9762\u5411Apple Silicon\u548CLinux\u7684SSD\u6D41\u5F0FMoE\u63A8\u7406\u5F15\u64CE',
      'tools.moestream.ssd': '\u4EC5\u97004GB RAM\u5373\u53EF\u8FD0\u884C80B\u6A21\u578B\uFF08NVMe SSD\u6D41\u5F0F\u52A0\u8F7D\uFF09',
      'tools.moestream.adaptive': '\u5C42\u81EA\u9002\u5E94\u526A\u679D\u652F\u6301\uFF08experts_per_layer\uFF09',
      'tools.moestream.speed': 'Q4\u91CF\u5316matmul \u2014 +79%\u52A0\u901F',
      'tools.moestream.metal': 'Metal GPU + CPU\u6DF7\u5408\u63A8\u7406',
      'tools.moestream.github': '\u5728GitHub\u67E5\u770B',
      'model.lossless': '\u65E0\u635F',
      'model.50pruned': '50% \u526A\u51CF',
      'model.japanese': '\u65E5\u8BED',
      'model.experimental': '\u5B9E\u9A8C\u6027',
      'model.gptoss28.desc': '所有基准测试零质量损失。MMLU 78%、HumanEval 78%、GSM8K 92%。适合16GB RAM。',
      'model.qwen80b.desc': '80B模型压缩至24GB。MMLU 72%。44%剪枝（27.7 GB）时：LCB Easy 83%、HumanEval 72%。',
      'model.jp30b.desc': '\u8BED\u8A00\u611F\u77E5\u526A\u679D\u4FDD\u7559\u65E5\u8BED\u8D28\u91CF\u3002Thinking-ON: MMLU 79%, JA 90%\u3002',
      'model.zerobias.desc': '\u8DEF\u7531\u5668\u4F18\u5316\u5728\u526A\u679D\u60AC\u5D16\u5904\u6062\u590D\u8D28\u91CF\u3002\u4E13\u5BB6\u51CF\u5C1115.6%\uFF0C\u4EC5-1pp\u3002',
      'tech.title': '\u5DE5\u4F5C\u539F\u7406',
      'tech.desc': '\u4E13\u5BB6\u526A\u679D\uFF0C\u800C\u975E\u6FC0\u8FDB\u91CF\u5316\u3002',
      'tech.calibration': '\u57FA\u4E8E\u6821\u51C6\u7684\u8BC4\u5206',
      'tech.calibration.desc': '\u901A\u8FC7\u591A\u6837\u5316\u5DE5\u4F5C\u8D1F\u8F7D\u7684\u5B9E\u9645\u63A8\u7406\u6765\u8861\u91CF\u4E13\u5BB6\u91CD\u8981\u6027\u3002\u6BD4\u9759\u6001\u6743\u91CD\u5206\u6790\u7CBE\u786E\u5F97\u591A\u3002',
      'tech.adaptive': '\u5C42\u81EA\u9002\u5E94\u5206\u914D',
      'tech.adaptive.desc': '\u6BCF\u5C42\u52A8\u6001\u4FDD\u7559\u4E0D\u540C\u6570\u91CF\u7684\u4E13\u5BB6\u3002\u5BF9\u526A\u679D\u654F\u611F\u7684\u5C42\u4F18\u5148\u4FDD\u6301\u8D28\u91CF\u3002',
      'tech.language': '\u8BED\u8A00\u611F\u77E5\u4F18\u5316',
      'tech.language.desc': '\u81EA\u52A8\u68C0\u6D4B\u5E76\u4FDD\u62A4\u8BED\u8A00\u4E13\u4E1A\u4E13\u5BB6\u3002\u538B\u7F29\u65F6\u4FDD\u7559\u65E5\u8BED\u3001\u4E2D\u6587\u7B49\u8BED\u8A00\u80FD\u529B\u3002',
      'tech.zerobias': 'Zerobias\u8DEF\u7531\u4F18\u5316',
      'tech.zerobias.desc': '\u526A\u679D\u540E\u7684\u8DEF\u7531\u5668\u504F\u7F6E\u6821\u6B63\u6269\u5C55\u4E86\u65E0\u635F\u538B\u7F29\u7684\u8FB9\u754C\u3002\u96F6\u6210\u672C\uFF0C\u65E0\u9700\u91CD\u65B0\u8BAD\u7EC3\u3002',
      'comp.title': '\u4E13\u5BB6\u526A\u679D vs \u91CF\u5316',
      'comp.pruning': '\u4E13\u5BB6\u526A\u679D',
      'comp.quantization': 'Q2\u91CF\u5316',
      'comp.approach': '\u65B9\u6CD5',
      'comp.pruning.approach': '\u5B8C\u5168\u79FB\u9664\u5197\u4F59\u4E13\u5BB6',
      'comp.quant.approach': '\u964D\u4F4E\u6240\u6709\u6743\u91CD\u7684\u7CBE\u5EA6',
      'comp.quality': '\u8D28\u91CF\u5F71\u54CD',
      'comp.pruning.quality': '\u6709\u9488\u5BF9\u6027\uFF0C\u5F71\u54CD\u6700\u5C0F',
      'comp.quant.quality': '\u5747\u5300\u9000\u5316',
      'comp.24gb': '~24 GB\u6A21\u578B\u8D28\u91CF',
      'comp.pruning.24gb': 'MMLU 72%',
      'comp.quant.24gb': 'MMLU ~55\u201360%',
      'comp.precision': '\u6B8B\u4F59\u7CBE\u5EA6',
      'comp.pruning.precision': 'Q4\u5168\u7CBE\u5EA6',
      'comp.quant.precision': '2\u4F4D\u7CBE\u5EA6',
      'vision.title': '未来展望',
      'vision.desc': '专家剪枝只是开始。以下是即将成为可能的事情。',
      'vision.hardware': '消费级硬件',
      'vision.hardware.desc': '在24GB RAM的笔记本电脑上运行前沿MoE模型 \u2014 无需万元级服务器GPU。让最强AI触手可及。',
      'vision.energy': '更低能耗',
      'vision.energy.desc': '更少的专家意味着每个token更少的计算量。数据中心可以用一半的算力提供同等质量 \u2014 大规模节能。',
      'vision.rl': '剪枝后强化学习',
      'vision.rl.desc': '剪枝模型为定向强化学习创造了空间。相同大小，可能更好的性能 \u2014 不仅是质量保持，更是质量提升。',
      'vision.edge': '边缘部署',
      'vision.edge.desc': 'MoE模型可在设备端推理中实际使用。私密、快速、离线 \u2014 将专家级AI带到手机和嵌入式系统。',
      'cta.title': '诞生于MacBook。由你资助。',
      'cta.detail': '这项研究的全部 \u2014 22个阶段、4个模型、50多次实验 \u2014 在一台MacBook Pro（M4 Pro、24GB RAM、512GB SSD）上完成。没有数据中心。没有企业资助。',
      'cta.funding': '您的支持直接用于：在更大模型上测试的GPU资源、全面基准测试的云计算，以及继续开源研究的时间。',
      'cta.button': '在Ko-fi上支持',
      'footer.name': 'GOBA-AI-Labs',
    },
    'zh-TW': {
      'nav.models': '\u6A21\u578B',
      'nav.technology': '\u6280\u8853',
      'nav.huggingface': 'HuggingFace',
      'nav.support': '\u652F\u6301\u6211\u5011',
      'hero.title': '\u5C08\u5BB6\u6E1B\u534A\u3002<br>\u54C1\u8CEA\u4E0D\u8B8A\u3002',
      'hero.subtitle': '\u6211\u5011\u79FB\u9664MoE\u6A21\u578B\u591A\u905450%\u7684\u5C08\u5BB6\uFF0C\u540C\u6642\u4FDD\u6301\u57FA\u6E96\u6E2C\u8A66\u5206\u6578\u3002\u5DF2\u5728\u6700\u5927 80B \u6A21\u578B\u4E0A\u9A57\u8B49 \u2014 \u76EE\u6A19\u662F\u8B93400B+\u6A21\u578B\u5728\u6D88\u8CBB\u7D1A\u786C\u9AD4\u4E0A\u904B\u884C\u3002',
      'hero.browse': '\u700F\u89BD\u6A21\u578B',
      'hero.huggingface': 'HuggingFace',
      'hero.stat': 'Qwen3-Coder-Next (80B) \u2014 50% expert pruning, MMLU 72%',
      'nav.tools': '\u5DE5\u5177',
      'models.title': 'PrunedHub \u6A21\u578B',
      'models.desc': '\u526A\u679D\u5F8C\u7684MoE\u6A21\u578B\u3002GPT-OSS\u652F\u63F4llama.cpp\uFF0CQwen3\u9700\u8981moe-stream\u3002',
      'tools.title': '\u63A8\u7406\u5DE5\u5177',
      'tools.desc': '\u5728\u6D88\u8CBB\u7D1A\u786C\u9AD4\u4E0A\u57F7\u884C\u526A\u679D\u5F8C\u7684MoE\u6A21\u578B\u3002',
      'tools.moestream.tagline': '\u9762\u5411Apple Silicon\u548CLinux\u7684SSD\u6D41\u5F0FMoE\u63A8\u7406\u5F15\u64CE',
      'tools.moestream.ssd': '\u50C54GB RAM\u5373\u53EF\u57F7\u884C80B\u6A21\u578B\uFF08NVMe SSD\u6D41\u5F0F\u8F09\u5165\uFF09',
      'tools.moestream.adaptive': '\u5C64\u81EA\u9069\u61C9\u526A\u679D\u652F\u63F4\uFF08experts_per_layer\uFF09',
      'tools.moestream.speed': 'Q4\u91CF\u5316matmul \u2014 +79%\u52A0\u901F',
      'tools.moestream.metal': 'Metal GPU + CPU\u6DF7\u5408\u63A8\u7406',
      'tools.moestream.github': '\u5728GitHub\u67E5\u770B',
      'model.lossless': '\u7121\u640D',
      'model.50pruned': '50% \u524A\u6E1B',
      'model.japanese': '\u65E5\u8A9E',
      'model.experimental': '\u5BE6\u9A57\u6027',
      'model.gptoss28.desc': '所有基準測試零品質損失。MMLU 78%、HumanEval 78%、GSM8K 92%。適合16GB RAM。',
      'model.qwen80b.desc': '80B模型壓縮至24GB。MMLU 72%。44%剪枝（27.7 GB）時：LCB Easy 83%、HumanEval 72%。',
      'model.jp30b.desc': '\u8A9E\u8A00\u611F\u77E5\u526A\u679D\u4FDD\u7559\u65E5\u8A9E\u54C1\u8CEA\u3002Thinking-ON: MMLU 79%, JA 90%\u3002',
      'model.zerobias.desc': '\u8DEF\u7531\u5668\u512A\u5316\u5728\u526A\u679D\u61F8\u5D16\u8655\u6062\u5FA9\u54C1\u8CEA\u3002\u5C08\u5BB6\u6E1B\u5C1115.6%\uFF0C\u50C5-1pp\u3002',
      'tech.title': '\u5DE5\u4F5C\u539F\u7406',
      'tech.desc': '\u5C08\u5BB6\u526A\u679D\uFF0C\u800C\u975E\u6FC0\u9032\u91CF\u5316\u3002',
      'tech.calibration': '\u57FA\u65BC\u6821\u6E96\u7684\u8A55\u5206',
      'tech.calibration.desc': '\u900F\u904E\u591A\u6A23\u5316\u5DE5\u4F5C\u8CA0\u8F09\u7684\u5BE6\u969B\u63A8\u7406\u4F86\u8861\u91CF\u5C08\u5BB6\u91CD\u8981\u6027\u3002\u6BD4\u975C\u614B\u6B0A\u91CD\u5206\u6790\u7CBE\u78BA\u5F97\u591A\u3002',
      'tech.adaptive': '\u5C64\u81EA\u9069\u61C9\u5206\u914D',
      'tech.adaptive.desc': '\u6BCF\u5C64\u52D5\u614B\u4FDD\u7559\u4E0D\u540C\u6578\u91CF\u7684\u5C08\u5BB6\u3002\u5C0D\u526A\u679D\u654F\u611F\u7684\u5C64\u512A\u5148\u4FDD\u6301\u54C1\u8CEA\u3002',
      'tech.language': '\u8A9E\u8A00\u611F\u77E5\u512A\u5316',
      'tech.language.desc': '\u81EA\u52D5\u6AA2\u6E2C\u4E26\u4FDD\u8B77\u8A9E\u8A00\u5C08\u696D\u5C08\u5BB6\u3002\u58D3\u7E2E\u6642\u4FDD\u7559\u65E5\u8A9E\u3001\u4E2D\u6587\u7B49\u8A9E\u8A00\u80FD\u529B\u3002',
      'tech.zerobias': 'Zerobias\u8DEF\u7531\u512A\u5316',
      'tech.zerobias.desc': '\u526A\u679D\u5F8C\u7684\u8DEF\u7531\u5668\u504F\u7F6E\u6821\u6B63\u64F4\u5C55\u4E86\u7121\u640D\u58D3\u7E2E\u7684\u908A\u754C\u3002\u96F6\u6210\u672C\uFF0C\u7121\u9700\u91CD\u65B0\u8A13\u7DF4\u3002',
      'comp.title': '\u5C08\u5BB6\u526A\u679D vs \u91CF\u5316',
      'comp.pruning': '\u5C08\u5BB6\u526A\u679D',
      'comp.quantization': 'Q2\u91CF\u5316',
      'comp.approach': '\u65B9\u6CD5',
      'comp.pruning.approach': '\u5B8C\u5168\u79FB\u9664\u5197\u9918\u5C08\u5BB6',
      'comp.quant.approach': '\u964D\u4F4E\u6240\u6709\u6B0A\u91CD\u7684\u7CBE\u5EA6',
      'comp.quality': '\u54C1\u8CEA\u5F71\u97FF',
      'comp.pruning.quality': '\u6709\u91DD\u5C0D\u6027\uFF0C\u5F71\u97FF\u6700\u5C0F',
      'comp.quant.quality': '\u5747\u52FB\u9000\u5316',
      'comp.24gb': '~24 GB\u6A21\u578B\u54C1\u8CEA',
      'comp.pruning.24gb': 'MMLU 72%',
      'comp.quant.24gb': 'MMLU ~55\u201360%',
      'comp.precision': '\u6B98\u9918\u7CBE\u5EA6',
      'comp.pruning.precision': 'Q4\u5168\u7CBE\u5EA6',
      'comp.quant.precision': '2\u4F4D\u5143\u7CBE\u5EA6',
      'vision.title': '未來展望',
      'vision.desc': '專家剪枝只是開始。以下是即將成為可能的事情。',
      'vision.hardware': '消費級硬體',
      'vision.hardware.desc': '在24GB RAM的筆記型電腦上執行前沿MoE模型 \u2014 無需萬元級伺服器GPU。讓最強AI觸手可及。',
      'vision.energy': '更低能耗',
      'vision.energy.desc': '更少的專家意味著每個token更少的計算量。資料中心可以用一半的算力提供同等品質 \u2014 大規模節能。',
      'vision.rl': '剪枝後強化學習',
      'vision.rl.desc': '剪枝模型為定向強化學習創造了空間。相同大小，可能更好的效能 \u2014 不僅是品質保持，更是品質提升。',
      'vision.edge': '邊緣部署',
      'vision.edge.desc': 'MoE模型可在裝置端推理中實際使用。私密、快速、離線 \u2014 將專家級AI帶到手機和嵌入式系統。',
      'cta.title': '誕生於MacBook。由你資助。',
      'cta.detail': '這項研究的全部 \u2014 22個階段、4個模型、50多次實驗 \u2014 在一台MacBook Pro（M4 Pro、24GB RAM、512GB SSD）上完成。沒有資料中心。沒有企業資助。',
      'cta.funding': '您的支持直接用於：在更大模型上測試的GPU資源、全面基準測試的雲端運算，以及繼續開源研究的時間。',
      'cta.button': '在Ko-fi上支持',
      'footer.name': 'GOBA-AI-Labs',
    },
    ko: {
      'nav.models': '\uBAA8\uB378',
      'nav.technology': '\uAE30\uC220',
      'nav.huggingface': 'HuggingFace',
      'nav.support': '\uD6C4\uC6D0\uD558\uAE30',
      'hero.title': '\uC804\uBB38\uAC00 \uC808\uBC18.<br>\uD488\uC9C8 \uADF8\uB300\uB85C.',
      'hero.subtitle': 'MoE \uBAA8\uB378\uC758 \uC804\uBB38\uAC00\uB97C \uCD5C\uB300 50% \uC81C\uAC70\uD558\uBA74\uC11C \uBCA4\uCE58\uB9C8\uD06C \uC810\uC218\uB97C \uC720\uC9C0\uD569\uB2C8\uB2E4. \uCD5C\uB300 80B \uBAA8\uB378\uC5D0\uC11C \uAC80\uC99D \uC644\uB8CC \u2014 \uBAA9\uD45C\uB294 400B+ \uBAA8\uB378\uC744 \uC18C\uBE44\uC790 \uD558\uB4DC\uC6E8\uC5B4\uC5D0\uC11C \uC2E4\uD589\uD558\uB294 \uAC83\uC785\uB2C8\uB2E4.',
      'hero.browse': '\uBAA8\uB378 \uBCF4\uAE30',
      'hero.huggingface': 'HuggingFace',
      'hero.stat': 'Qwen3-Coder-Next (80B) \u2014 50% expert pruning, MMLU 72%',
      'nav.tools': '\uB3C4\uAD6C',
      'models.title': 'PrunedHub \uBAA8\uB378',
      'models.desc': '\uD504\uB8E8\uB2DD\uB41C MoE \uBAA8\uB378. GPT-OSS\uB294 llama.cpp \uD638\uD658, Qwen3\uB294 moe-stream \uD544\uC694.',
      'tools.title': '\uCD94\uB860 \uB3C4\uAD6C',
      'tools.desc': '\uC18C\uBE44\uC790 \uD558\uB4DC\uC6E8\uC5B4\uC5D0\uC11C \uD504\uB8E8\uB2DD\uB41C MoE \uBAA8\uB378\uC744 \uC2E4\uD589.',
      'tools.moestream.tagline': 'Apple Silicon \uBC0F Linux\uC6A9 SSD \uC2A4\uD2B8\uB9AC\uBC0D MoE \uCD94\uB860 \uC5D4\uC9C4',
      'tools.moestream.ssd': '4GB RAM\uC73C\uB85C 80B \uBAA8\uB378 \uC2E4\uD589 (NVMe SSD \uC2A4\uD2B8\uB9AC\uBC0D)',
      'tools.moestream.adaptive': '\uB808\uC774\uC5B4 \uC801\uC751\uD615 \uD504\uB8E8\uB2DD \uC9C0\uC6D0 (experts_per_layer)',
      'tools.moestream.speed': 'Q4 \uC591\uC790\uD654 matmul \u2014 +79% \uC18D\uB3C4 \uD5A5\uC0C1',
      'tools.moestream.metal': 'Metal GPU + CPU \uD558\uC774\uBE0C\uB9AC\uB4DC \uCD94\uB860',
      'tools.moestream.github': 'GitHub\uC5D0\uC11C \uBCF4\uAE30',
      'model.lossless': '\uBB34\uC190\uC2E4',
      'model.50pruned': '50% \uC0AD\uAC10',
      'model.japanese': '\uC77C\uBCF8\uC5B4',
      'model.experimental': '\uC2E4\uD5D8\uC801',
      'model.gptoss28.desc': '모든 벤치마크에서 품질 손실 제로. MMLU 78%, HumanEval 78%, GSM8K 92%. 16GB RAM에 수용.',
      'model.qwen80b.desc': '80B 모델을 24GB로 압축. MMLU 72%. 44% 프루닝 (27.7 GB) 시: LCB Easy 83%, HumanEval 72%.',
      'model.jp30b.desc': '\uC5B8\uC5B4 \uC778\uC2DD \uD504\uB8E8\uB2DD\uC73C\uB85C \uC77C\uBCF8\uC5B4 \uD488\uC9C8 \uC720\uC9C0. Thinking-ON: MMLU 79%, JA 90%.',
      'model.zerobias.desc': '\uB77C\uC6B0\uD130 \uCD5C\uC801\uD654\uB85C \uD504\uB8E8\uB2DD \uD074\uB9AC\uD504\uC5D0\uC11C \uD488\uC9C8 \uBCF5\uAD6C. \uC804\uBB38\uAC00 15.6% \uAC10\uC18C\uB85C -1pp.',
      'tech.title': '\uC791\uB3D9 \uC6D0\uB9AC',
      'tech.desc': '\uACF5\uACA9\uC801\uC778 \uC591\uC790\uD654\uAC00 \uC544\uB2CC, \uC804\uBB38\uAC00 \uD504\uB8E8\uB2DD.',
      'tech.calibration': '\uCE98\uB9AC\uBE0C\uB808\uC774\uC158 \uAE30\uBC18 \uC2A4\uCF54\uC5B4\uB9C1',
      'tech.calibration.desc': '\uB2E4\uC591\uD55C \uC6CC\uD06C\uB85C\uB4DC\uC5D0\uC11C\uC758 \uC2E4\uC81C \uCD94\uB860\uC744 \uD1B5\uD574 \uC804\uBB38\uAC00 \uC911\uC694\uB3C4\uB97C \uCE21\uC815. \uC815\uC801 \uAC00\uC911\uCE58 \uBD84\uC11D\uBCF4\uB2E4 \uD6E8\uC52C \uC815\uD655.',
      'tech.adaptive': '\uB808\uC774\uC5B4 \uC801\uC751\uD615 \uD560\uB2F9',
      'tech.adaptive.desc': '\uAC01 \uB808\uC774\uC5B4\uAC00 \uB3D9\uC801\uC73C\uB85C \uACB0\uC815\uB41C \uC218\uC758 \uC804\uBB38\uAC00\uB97C \uC720\uC9C0. \uD504\uB8E8\uB2DD\uC5D0 \uBBFC\uAC10\uD55C \uB808\uC774\uC5B4\uC758 \uD488\uC9C8\uC744 \uC6B0\uC120 \uBCF4\uC804.',
      'tech.language': '\uC5B8\uC5B4 \uC778\uC2DD \uCD5C\uC801\uD654',
      'tech.language.desc': '\uC5B8\uC5B4 \uC804\uBB38 \uC804\uBB38\uAC00\uB97C \uC790\uB3D9 \uAC10\uC9C0\uD558\uACE0 \uBCF4\uD638. \uC555\uCD95 \uC2DC \uC77C\uBCF8\uC5B4, \uC911\uAD6D\uC5B4 \uB4F1\uC758 \uC5B8\uC5B4 \uB2A5\uB825\uC744 \uBCF4\uC874.',
      'tech.zerobias': 'Zerobias \uB77C\uC6B0\uD130 \uCD5C\uC801\uD654',
      'tech.zerobias.desc': '\uD504\uB8E8\uB2DD \uD6C4 \uB77C\uC6B0\uD130 \uBC14\uC774\uC5B4\uC2A4 \uBCF4\uC815\uC73C\uB85C \uBB34\uC190\uC2E4 \uC555\uCD95 \uD55C\uACC4\uB97C \uD655\uC7A5. \uBE44\uC6A9 \uC81C\uB85C, \uC7AC\uD559\uC2B5 \uBD88\uD544\uC694.',
      'comp.title': '\uC804\uBB38\uAC00 \uD504\uB8E8\uB2DD vs \uC591\uC790\uD654',
      'comp.pruning': '\uC804\uBB38\uAC00 \uD504\uB8E8\uB2DD',
      'comp.quantization': 'Q2 \uC591\uC790\uD654',
      'comp.approach': '\uC811\uADFC\uBC95',
      'comp.pruning.approach': '\uC911\uBCF5 \uC804\uBB38\uAC00\uB97C \uC644\uC804\uD788 \uC81C\uAC70',
      'comp.quant.approach': '\uBAA8\uB4E0 \uAC00\uC911\uCE58\uC758 \uC815\uBC00\uB3C4 \uAC10\uC18C',
      'comp.quality': '\uD488\uC9C8 \uC601\uD5A5',
      'comp.pruning.quality': '\uD0C0\uAC9F\uD615, \uCD5C\uC18C\uD55C\uC758 \uC601\uD5A5',
      'comp.quant.quality': '\uADE0\uC77C\uD55C \uC800\uD558',
      'comp.24gb': '~24 GB \uBAA8\uB378 \uD488\uC9C8',
      'comp.pruning.24gb': 'MMLU 72%',
      'comp.quant.24gb': 'MMLU ~55\u201360%',
      'comp.precision': '\uC794\uC5EC \uC815\uBC00\uB3C4',
      'comp.pruning.precision': 'Q4 \uC804\uCCB4 \uC815\uBC00\uB3C4',
      'comp.quant.precision': '2\uBE44\uD2B8 \uC815\uBC00\uB3C4',
      'vision.title': '미래 전망',
      'vision.desc': '전문가 프루닝은 시작에 불과합니다. 앞으로 가능해지는 것들.',
      'vision.hardware': '소비자 하드웨어',
      'vision.hardware.desc': '24GB RAM 노트북에서 최첨단 MoE 모델 실행 \u2014 고가의 서버 GPU 불필요. 가장 강력한 AI에 대한 접근을 민주화.',
      'vision.energy': '낮은 에너지 비용',
      'vision.energy.desc': '더 적은 전문가는 토큰당 더 적은 연산을 의미. 데이터 센터가 절반의 컴퓨팅으로 동일한 품질을 제공 \u2014 대규모 전력 절감.',
      'vision.rl': '프루닝 후 강화학습',
      'vision.rl.desc': '프루닝된 모델은 타겟 강화학습의 여지를 만듭니다. 같은 크기, 잠재적으로 더 나은 성능 \u2014 품질 보존이 아닌 품질 증폭.',
      'vision.edge': '엣지 배포',
      'vision.edge.desc': 'MoE 모델이 온디바이스 추론에서 실용적으로. 프라이빗, 빠르고, 오프라인 \u2014 전문가 수준의 AI를 스마트폰과 임베디드 시스템으로.',
      'cta.title': 'MacBook에서 탄생. 여러분이 후원.',
      'cta.detail': '이 연구의 전부 \u2014 22단계, 4개 모델, 50회 이상의 실험 \u2014 는 한 대의 MacBook Pro (M4 Pro, 24GB RAM, 512GB SSD)에서 수행되었습니다. 데이터 센터 없이. 기업 자금 없이.',
      'cta.funding': '여러분의 후원은 더 큰 모델 테스트를 위한 GPU, 종합 벤치마크를 위한 클라우드 컴퓨팅, 그리고 오픈소스 연구를 지속하는 시간에 직접 사용됩니다.',
      'cta.button': 'Ko-fi에서 후원',
      'footer.name': 'GOBA-AI-Labs',
    },
  };

  const langLabels = { en: 'EN', ja: 'JA', 'zh-CN': 'ZH', 'zh-TW': 'TW', ko: 'KO' };

  function getPreferred() {
    var saved = localStorage.getItem(KEY);
    if (saved && translations[saved]) return saved;
    var nav = (navigator.language || '').toLowerCase();
    if (nav.startsWith('ja')) return 'ja';
    if (nav === 'zh-tw' || nav === 'zh-hant') return 'zh-TW';
    if (nav.startsWith('zh')) return 'zh-CN';
    if (nav.startsWith('ko')) return 'ko';
    return 'en';
  }

  function applyLang(lang) {
    if (!translations[lang]) lang = 'en';
    var t = translations[lang];
    document.querySelectorAll('[data-i18n]').forEach(function (el) {
      var key = el.getAttribute('data-i18n');
      if (t[key] !== undefined) {
        el.innerHTML = t[key];
      }
    });
    document.documentElement.lang = lang === 'zh-CN' ? 'zh-Hans' : lang === 'zh-TW' ? 'zh-Hant' : lang;
    localStorage.setItem(KEY, lang);
    // Update active state in switcher
    document.querySelectorAll('.lang-option').forEach(function (btn) {
      btn.classList.toggle('active', btn.getAttribute('data-lang') === lang);
    });
  }

  document.addEventListener('DOMContentLoaded', function () {
    var lang = getPreferred();
    applyLang(lang);

    // Bind language switcher buttons
    document.querySelectorAll('.lang-option').forEach(function (btn) {
      btn.addEventListener('click', function () {
        applyLang(btn.getAttribute('data-lang'));
      });
    });
  });

  // Export for external use
  window.gobaI18n = { apply: applyLang, get: getPreferred };
})();
